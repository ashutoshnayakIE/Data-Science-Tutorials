{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Primer on LIME and SHAP: Interpretable Machine Learning</h1><br>\n",
    "Interpretability of machine learning models is as important as performance metrics of the model. While it is very difficult to solve the problem of global fidelity, LIME and SHAP aim for local fidelity so that they can explain the models locally (more discussion on this later). LIME and SHAP are also model agnostic which makes them more relevant (for example the method of variable importance in random forests where the first variable on which data is spli is more important, this cannot be implemented in a linear regression).\n",
    "\n",
    "*This blog is about trying to understand what are we actually doing when using LIME and SHAP library*\n",
    "\n",
    "References (the blog is drawn from the these papers). While this blog gives an intuitive explanation of LIME and SHAP, please read the papers below for advanced learning. <br>\n",
    "[1] LIME: https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf <br>\n",
    "[2] SHAP: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf <br>\n",
    "\n",
    "The following blogs are in the same line as this blog, explaining LIME and SHAP at a lower level: <br>\n",
    "[3] https://christophm.github.io/interpretable-ml-book/shapley.html (must read before LIME)<br>\n",
    "[4] https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83 (must read before SHAP)<br>\n",
    "\n",
    "Additional sources on why should we consider interpretability: <br>\n",
    "[5] H20 video: https://www.youtube.com/watch?v=Q8rTrmqUQsU&t=1s <br>\n",
    "[6] Scott's blog: https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27 <br>\n",
    "\n",
    "This blog is not about how to use the LIME and SHAP library or how to interpret their results. You can find them at: <br>\n",
    "[7] https://towardsdatascience.com/how-to-avoid-the-machine-learning-blackbox-with-shap-da567fc64a8b <br>\n",
    "[8] https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Core Idea of local fidelity </h3> \n",
    "\n",
    "The models have to be locally faithful, that is they could explain the model predictions faithfully for individual data points. For example, if we model age and gender as a predictor for salary, age might have a positive coeficient for the population (global fidelity) but for a younger progessionals at silicon valley, age might not be that positive or even negative coefficient atlocal fidelity). Thus global and local fidelity differ with each other. To get a local fidelity, a data point $x$ is converted into an interpretable representation.\n",
    "\n",
    "Interpretable representation of a data point is representing a data point in a multidimensional binary vector $x'=\\{0,1\\}^D$ where $x$ could be a $d$ dimension vector and its interpretable representation is a $D$ dimension vector. For example, if $x = \\{x_1,x_2,..,x_d\\}$, then interpretable format will be $z = \\{z_1,z_2,z_3,...,z_D\\}$ where $z_i=\\{0,1\\}$ so we must create our rules to convert $x$ into its interpretable format $z$ which is a $D$ dimension vector with binary variables.\n",
    "\n",
    "Example: For linear regression, we can convert $x$ into binary format representing them as absence or presence of a variable $x_1$. *Note that the value of the variable is not considered here as it is simply converted to binary, however, it can be converted into binary by converting a value say 10 by <12 or >= 12*. For an image, we can consider a chunk of pixels with similar pixel value (example: representing nose of a cat in an image) and convert it into interpretable representation as the absence or presence of cat's nose in the image. Example from [1] is attached here.\n",
    "\n",
    "<img src="lime.png">\n",
    "\n",
    "Let $f$ be the model eg. linear regression, let $g$ be the model which works with interpretable reresentation of the data (binary form). Let the model complexity be represented by $\\omega(g)$, it could be the number of variables with non-zero coefficients in linear regression or depth of tree in random forest. Let the interpretable representation be $z'$ and $\\pi_x(z')$ be the proximity measure of $x'$ and $z'$. We want to find $g$ so as to minimize $\\eta(x) = L(f,g,\\pi_x) + \\omega(g)$ where $L$ is a fidelity function, $g$ is a interpreable model and $\\omega$ is a complexity measure. $L$ is a locality aware loss and no assumption is made on $f$ as we want a model agnostic interpreation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sampling for local interpretation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can convert a data $x$ into $x'$, we will sample $z'$ from $x'$. An example could be dropping one dimension of of the $D$ dimensions of $x'$ and making it 0 to create a sample $z'$. $\\pi_x(z')$ is the measure of how close $x'$ is from $z'$ (in the example, it is not too far as we dropped just one of the D dimensions). These draws are uniformly sampled and the weight given to each sample $z'$ is $\\pi_x(z')$. \n",
    "\n",
    "Here each sample $z'$ is a neighbor of x' as z' is obtained from removing some of the elements from interpretable representation of $x$ given by $x'$. We can obtain $z$ from $z'$ as we know the rules as how we converted the data into a binary data. If $f(x)$ was the model prediction (e.g.probability that the image contains a cat), we can obtain $f(z)$ as the probability that the image contains the cat in an image that is missing some of the original pixels as a group of pixels have been turned off to get $z$. These turned off pixels can be replaced by average of their neighboring pixels or 0 (this is model dependent and upto the user)\n",
    "\n",
    "Now we can run a linear regression model in the locality of the data point $x$. The interpretation of the linear regression model may not be valid globally as it may not impact each data point in the same manner, but we can be confident that locally it explains $x$ well. A simple linear regression model is considered to map z -> f(z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Optimization model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to optimize $\\eta(x)$. Let $g(z') = w_gz'$ and $\\pi_x(z)$ is given by $exp (dist(x,z)/\\sigma)$ some exponential kernel with width $\\sigma$ where $dist(x,z)$ could be the euclidean distance or manhatten distance or cosine distance.\n",
    "\n",
    "Let $L(f,g,\\pi_x) = \\sum_{z,z'}\\pi_x(z)(f(z)-g(z'))^2$. Thus it is weighted by the difference between $x$ and $z$ and it aims to optimize for $w_g$ (and thus finds the best $g$ among all possible $g$ in the hypothesis set $G$). The weights $w_g$ is the LIME value for that particular interpretable variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Something more </h3>\n",
    "\n",
    "LIME is an additive attribution model as $g(z')$ is additive (total impact is sum of the individual impact of the different dimensions of $x'$). LIME has been criticized for its lack of stabiity [8]. Also, defining a good neighborhood of any given data point $x$ is difficult (and depends on the dist function used). Two more important properties include consistency (changing the model does not decrease the attribution of a variable if its contribution increases or remains the same) and missingness (missing variable should have 0 attribution).\n",
    "\n",
    "Shapley value from game theory (theoretically proven) fulfils all the three properties. An easy example of shapley value is shown in the video: https://www.youtube.com/watch?v=w9O0fkfMkx0. For example, in linear regression, if we need to find the contribution of a variable, we can keep adding (or removing a variable) from the model. However, the sequence matters in which the variable is added or removed from the model, particularly when there is multicollinearity. Shapley values is based on this contribution but it uses all the possible sequences for calculating the contribution of an individual variable to achieve fairness. Using all possible sequences and taking an average (as explained in the video) provide an estimate of the contribution of the variable.\n",
    "\n",
    "*Thus the shapley value of a variable is estimated marginal contribution of the feature. SHAP is based on shapley values*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> How is SHAP different from LIME? </h1>\n",
    "\n",
    "In LIME, we sample $z$ by randomly dropping dimensions in $z'$ and building a linear regression model along the neighborhood of a data point $x$. We assume that \"explanation model $g$\" is linear and the weights of the linear regression is the variable contribution to the prediction. The linear regression is weighted by the distance between $x$ and $z$ so that samples closer to $x$ get higher weights. In SHAP, the again the samples are collected but not randomly (it could be sampled using MCMC so it is not entirely non-random). First, we obtain all the possible sequence(order) of variables. Samples are collected so that all these sequences are included in the sample. The number of samples grow exponentially with size that makes SHAP very time consuming. Once the samples are collected, contribution of each variable in each data point is obtained using the same model $f$, thus there is no need to build another linear regression model.\n",
    "\n",
    "While LIME is interpretable, SHAP has two more features which makes it more attractive: <br>\n",
    "1) consistency <br>\n",
    "2) stability <br>\n",
    "Shapley value guarantees fair distribution of contribution for each of the variable (LIME do not provide guarantee). LIME assumes that the local model is linear, SHAP does not have any such assumptions. SHAP value calculation is very time expensive (as it checks all the possible combinations: it does it through monte carlo simulations rather than brute force). SHAP value is <b>NOT</b> the difference between the prediction with and without a variable, rather it is contribution of a variable to the difference between the actual prediction and the mean prediction. In other words, SHAP is the changes in the expected model prediction when conditioned on that variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculating SHAP </h3> <br>\n",
    "While in LIME, we build a local fidelity model using linear regression, in SHAP we try to find the contribution of a variable by using the same model (no linear regression model) twice. In first, we include the variable of interest and in second we do not. Along with the variable of interest, we consider all possible combinations of other predictor variables. Taking the average over all possible combinations of the difference between the predictions of the two models gives us the SHAP value of the variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Final words</h3>\n",
    "\n",
    "I hope the background of what we are doing in LIME and SHAP is clearer. Both are model agnostic and the library is available for standard machine learning models. Due to its theoretical guarantees and simplicity, SHAP is widely used and may be more acceptable [5]. In LIME, we need to define how we are considering a \"neighbor\" due to the presence of $\\sigma$. Also, we build a linear local model which might not be linear in a very complicated decision surface (even at a local level). In SHAP, we can use the same model on which we trained using the training data. \n",
    "\n",
    "The variable importance at a global level is given by adding the absolute value of the SHAP values for each individual data point. Although SHAP uses all the variables, we can select some variables with higher variable importance, drop other variables and rerun SHAP if we want to reduce the number of variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
